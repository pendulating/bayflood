{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:51 - analysis-df-assembly - INFO - Modules loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import numpy as np \n",
    "import json \n",
    "from glob import glob \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(\"analysis-df-assembly\")\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "WGS='EPSG:4326'\n",
    "PROJ='EPSG:2263'\n",
    "\n",
    "import os \n",
    "\n",
    "logger.info(\"Modules loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERNAL_DATA = True \n",
    "\n",
    "USE_SMOOTHING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAR_NONE_RUN='../runs/icar_none/simulated_False/ahl_True/20241021-1038'\n",
    "ICAR_CHEATING_RUN='../runs/icar_cheating/simulated_False/ahl_True/20241022-1130'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:51 - analysis-df-assembly - INFO - Found 2 ICAR_NONE estimates and 3 ICAR_CHEATING estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ICAR_NONE_ESTIMATES = glob(f\"{ICAR_NONE_RUN}/estimate*.csv\")\n",
    "ICAR_CHEATING_ESTIMATES = glob(f\"{ICAR_CHEATING_RUN}/estimate*.csv\")\n",
    "logger.info(f\"Found {len(ICAR_NONE_ESTIMATES)} ICAR_NONE estimates and {len(ICAR_CHEATING_ESTIMATES)} ICAR_CHEATING estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_cheating_estimates = {} \n",
    "for f in ICAR_CHEATING_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_cheating_estimates[os.path.splitext(os.path.basename(f))[0]] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_none_estimates = {} \n",
    "for f in ICAR_NONE_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_none_estimates[os.path.splitext(os.path.basename(f)[0])] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:51 - analysis-df-assembly - INFO - Using smoothed estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_SMOOTHING: \n",
    "    icar_model_estimates = icar_cheating_estimates\n",
    "    logger.info(\"Using smoothed estimates.\")\n",
    "else:\n",
    "    icar_model_estimates = icar_none_estimates\n",
    "    logger.info(\"Using unsmoothed estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:51 - analysis-df-assembly - INFO - Loaded NYC CT shapefile with 2325 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson')\n",
    "\n",
    "\n",
    "TO_DROP = ['OBJECTID','BoroCode','CT2020','CDEligibil','NTA2020','CDTA2020','Shape__Area','Shape__Length','geometry']\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Loaded NYC CT shapefile with {len(ct_nyc.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CTLabel', 'BoroName', 'BoroCT2020', 'NTAName', 'CDTANAME', 'GEOID',\n",
       "       'PUMA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:52 - analysis-df-assembly - INFO - Loaded NYC CT (water clipped) shapefile with 2327 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc_clip = gpd.read_file('geo/data/ct-nyc-2020.geojson')\n",
    "logger.info(f\"Loaded NYC CT (water clipped) shapefile with {len(ct_nyc_clip.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:52 - analysis-df-assembly - INFO - Merged NYC CT shapefile with icar model estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_p_y'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_y'))\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_alop'))\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area_if_you_have_100_images'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_alop_100'))\n",
    "\n",
    "# drop empirical_estimate_* cols \n",
    "TO_DROP = [c for c in ct_nyc.columns if 'empirical_estimate_' in c]\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Merged NYC CT shapefile with icar model estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dp05_nyc_md = pd.read_json('demo/data/acs22_dp05_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "dp05_nyc_md = pd.json_normalize(dp05_nyc_md['variables']).set_index(dp05_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    dp05_nyc_md[f'desc_{i}'] = dp05_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP \n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "dp05_nyc_md = dp05_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "dp05_nyc_md = dp05_nyc_md[dp05_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows \n",
    "dp05_nyc_md = dp05_nyc_md.sort_index()\n",
    "\n",
    "# to csv \n",
    "dp05_nyc_md.to_csv('demo/data/acs22_dp05_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_population</th>\n",
       "      <th>nhl_white_alone</th>\n",
       "      <th>nhl_black_alone</th>\n",
       "      <th>hispanic_alone</th>\n",
       "      <th>nhl_asian_alone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>4446</td>\n",
       "      <td>1098</td>\n",
       "      <td>2000</td>\n",
       "      <td>1172</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>4870</td>\n",
       "      <td>83</td>\n",
       "      <td>1281</td>\n",
       "      <td>3109</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>6257</td>\n",
       "      <td>283</td>\n",
       "      <td>1559</td>\n",
       "      <td>4212</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>6177</td>\n",
       "      <td>106</td>\n",
       "      <td>2132</td>\n",
       "      <td>3507</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>2181</td>\n",
       "      <td>306</td>\n",
       "      <td>942</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>6374</td>\n",
       "      <td>2209</td>\n",
       "      <td>1568</td>\n",
       "      <td>1625</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>3674</td>\n",
       "      <td>289</td>\n",
       "      <td>1626</td>\n",
       "      <td>1469</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>5053</td>\n",
       "      <td>473</td>\n",
       "      <td>2388</td>\n",
       "      <td>1913</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>1133</td>\n",
       "      <td>109</td>\n",
       "      <td>421</td>\n",
       "      <td>394</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0           total_population nhl_white_alone nhl_black_alone hispanic_alone  \\\n",
       "tract_id                                                                      \n",
       "36005000100             4446            1098            2000           1172   \n",
       "36005000200             4870              83            1281           3109   \n",
       "36005000400             6257             283            1559           4212   \n",
       "36005001600             6177             106            2132           3507   \n",
       "36005001901             2181             306             942            842   \n",
       "...                      ...             ...             ...            ...   \n",
       "36085030302             6374            2209            1568           1625   \n",
       "36085031901             3674             289            1626           1469   \n",
       "36085031902             5053             473            2388           1913   \n",
       "36085032300             1133             109             421            394   \n",
       "36085990100                0               0               0              0   \n",
       "\n",
       "0           nhl_asian_alone  \n",
       "tract_id                     \n",
       "36005000100             123  \n",
       "36005000200             299  \n",
       "36005000400             103  \n",
       "36005001600             148  \n",
       "36005001901               0  \n",
       "...                     ...  \n",
       "36085030302             918  \n",
       "36085031901             224  \n",
       "36085031902             217  \n",
       "36085032300              21  \n",
       "36085990100               0  \n",
       "\n",
       "[2327 rows x 5 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp05_nyc = pd.read_json('demo/data/acs22_dp05.json', orient='records')\n",
    "\n",
    "dp05_nyc.columns = dp05_nyc.iloc[0]\n",
    "dp05_nyc = dp05_nyc[1:]\n",
    "\n",
    "dp05_nyc['tract_id'] = dp05_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "RACE_COLS = {\n",
    "    'DP05_0001E': 'total_population',\n",
    "    'DP05_0079E': 'nhl_white_alone', \n",
    "    'DP05_0080E': 'nhl_black_alone', \n",
    "    'DP05_0073E': 'hispanic_alone', \n",
    "    'DP05_0082E': 'nhl_asian_alone'\n",
    "}\n",
    "\n",
    "race_nyc = dp05_nyc[list(RACE_COLS.keys())]\n",
    "race_nyc.columns = race_nyc.columns.map(lambda x: RACE_COLS[x])\n",
    "race_nyc.index = dp05_nyc['tract_id']\n",
    "race_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.merge(race_nyc, left_on='GEOID', right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.set_index('GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson').to_crs(PROJ)[['GEOID','geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:53 - analysis-df-assembly - INFO - Loaded and processed Floodnet sensor data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FLOODNET \n",
    "floodnet_sensor = pd.read_csv('flooding/static/floodnet-flood-sensor-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_tide = pd.read_csv('flooding/static/floodnet-tide-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_weather = pd.read_csv('flooding/static/floodnet-weather-sep-2023.csv', engine='pyarrow')\n",
    "\n",
    "\n",
    "all_floodnet_sensors_geo = pd.concat([floodnet_sensor.groupby('deployment_id').first()[['lat','lon']].reset_index(), floodnet_tide.groupby('sensor_id').first()[['lat','lon']].reset_index(), floodnet_weather.groupby('sensor_id').first()[['lat','lon']].reset_index()], axis=0)\n",
    "\n",
    "all_floodnet_sensor_geo = gpd.GeoDataFrame(all_floodnet_sensors_geo, geometry=gpd.points_from_xy(all_floodnet_sensors_geo.lon, all_floodnet_sensors_geo.lat), crs='EPSG:4326').to_crs(2263)\n",
    "\n",
    "del floodnet_sensor, floodnet_tide, floodnet_weather\n",
    "\n",
    "logger.info(\"Loaded and processed Floodnet sensor data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:41:53 - analysis-df-assembly - INFO - Merged Floodnet sensor data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 13:41:53 - analysis-df-assembly - INFO - count    2325.000000\n",
      "mean        0.036129\n",
      "std         0.272811\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         6.000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the number of floodnet sensors in each census tract in gdf_ct_nyc, including tracts with 0 sensors\n",
    "ct_nyc['n_sensors'] = gpd.sjoin(gdf_ct_nyc, all_floodnet_sensor_geo).groupby('GEOID').size().reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged Floodnet sensor data with NYC CT shapefile.\")\n",
    "logger.info(ct_nyc['n_sensors'].describe().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEP STORMWATER \n",
    "dep_light = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Extreme Flood (3.66 inches per hr) with 2080 Sea Level Rise/NYC_Stormwater_Flood_Map_Extreme_Flood_3_66_inches_per_hr_with_2080_Sea_Level_Rise.gdb').to_crs(PROJ)\n",
    "dep_moderate = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Moderate Flood (2.13 inches per hr) with Current Sea Levels/NYC_Stormwater_Flood_Map_Moderate_Flood_2_13_inches_per_hr_with_Current_Sea_Levels.gdb').to_crs(PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {} \n",
    "for i, row in dep_light.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "\n",
    "# dataframe from dict \n",
    "dep_light_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "# flooding category is the first part of the index\n",
    "\n",
    "\n",
    "dep_light_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_light_flattened.crs = dep_light.crs\n",
    "\n",
    "dep_light_flattened['Flooding_Category'] = dep_light_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {}\n",
    "for i, row in dep_moderate.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "    \n",
    "# dataframe from dict\n",
    "dep_moderate_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "\n",
    "dep_moderate_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_moderate_flattened.crs = dep_moderate.crs\n",
    "\n",
    "dep_moderate_flattened['Flooding_Category'] = dep_moderate_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:42:49 - analysis-df-assembly - INFO - Merged DEP stormwater data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 13:42:49 - analysis-df-assembly - INFO - \n",
      "       dep_light_1_area  dep_light_2_area  dep_light_3_area  dep_moderate_1_area  dep_moderate_2_area\n",
      "count      2.325000e+03      2.325000e+03      2.325000e+03          2325.000000          2325.000000\n",
      "mean       1.704254e+05      1.305767e+05      1.375244e+05         33568.736899         17255.142646\n",
      "std        1.926640e+05      2.015365e+05      7.969362e+05         77947.720054         49174.975593\n",
      "min        0.000000e+00      0.000000e+00      0.000000e+00             0.000000             0.000000\n",
      "25%        4.756178e+04      1.274651e+04      0.000000e+00             0.000000             0.000000\n",
      "50%        1.176228e+05      5.511028e+04      0.000000e+00          5211.095209             0.000000\n",
      "75%        2.227261e+05      1.648135e+05      0.000000e+00         33654.810667          9705.671683\n",
      "max        1.653186e+06      2.586617e+06      1.279555e+07        886875.919586        660828.326796\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the total area of light and moderate flooding in each ct in gdf_ct_nyc \n",
    "dep_light_flattened_1 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 1]\n",
    "dep_light_flattened_2 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 2]\n",
    "dep_light_flattened_3 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 3]\n",
    "ct_nyc['dep_light_1_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_2_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_3_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_3, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "dep_moderate_flattened_1 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 1]\n",
    "dep_moderate_flattened_2 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 2]\n",
    "ct_nyc['dep_moderate_1_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_moderate_2_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged DEP stormwater data with NYC CT shapefile.\")\n",
    "\n",
    "logger.info(\"\\n\"+ct_nyc[['dep_light_1_area','dep_light_2_area','dep_light_3_area','dep_moderate_1_area','dep_moderate_2_area']].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:42:49 - analysis-df-assembly - INFO - Loaded and processed 311 data from September 29, 2023.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 311 from September 29 Flood \n",
    "nyc311_sep29 = pd.read_csv('flooding/data/nyc311_flooding_sep29.csv').dropna(subset=['latitude','longitude'])\n",
    "\n",
    "nyc311_sep29 = gpd.GeoDataFrame(nyc311_sep29, geometry=gpd.points_from_xy(nyc311_sep29.longitude, nyc311_sep29.latitude), crs=WGS).to_crs(PROJ)\n",
    "\n",
    "logger.info(\"Loaded and processed 311 data from September 29, 2023.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptor\n",
       "Sewer Backup (Use Comments) (SA)                    1081\n",
       "Street Flooding (SJ)                                 625\n",
       "Catch Basin Clogged/Flooding (Use Comments) (SC)     429\n",
       "Manhole Overflow (Use Comments) (SA1)                 35\n",
       "Highway Flooding (SH)                                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc311_sep29['descriptor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:42:53 - analysis-df-assembly - INFO - Merged 311 data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 13:42:53 - analysis-df-assembly - INFO - sewer_backup_311c                    1081\n",
      "street_flooding_311c                  625\n",
      "catch_basin_clogged/flooding_311c     429\n",
      "manhole_overflow_311c                  35\n",
      "highway_flooding_311c                   1\n",
      "dtype: int64\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for each unique val of descriptor, create a column in gdf_ct_nyc with the count of 311 calls of that descriptor type in each tract \n",
    "for descriptor in nyc311_sep29['descriptor'].unique():\n",
    "    # human-readable column name \n",
    "    # remove anything inside () \n",
    "    desc = descriptor.split('(')[0].strip()\n",
    "    desc = desc.lower().replace(' ', '_') + '_311c'\n",
    "\n",
    "    gdf_ct_nyc[desc] = gdf_ct_nyc['geometry'].apply(lambda x: nyc311_sep29[nyc311_sep29['descriptor'] == descriptor].within(x).sum())\n",
    "\n",
    "logger.info(\"Merged 311 data with NYC CT shapefile.\")\n",
    "logger.info(gdf_ct_nyc[[c for c in gdf_ct_nyc.columns if '_311c' in c]].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ct_nyc with gdf_ct_nyc\n",
    "ct_nyc = ct_nyc.merge(gdf_ct_nyc, on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-23 13:42:53 - analysis-df-assembly - SUCCESS - No N/A values found in columns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "COLS_ALLOWED_NA_VALS = ['empirical_estimate']\n",
    "def na_validation(df, cols_allowed_na_vals):\n",
    "    for c in df.columns:\n",
    "        if c in cols_allowed_na_vals:\n",
    "            continue\n",
    "        if df[c].isna().sum() > 0:\n",
    "            logger.error(f\"Column {c} has {df[c].isna().sum()} NA values.\")\n",
    "    else: \n",
    "        logger.success(\"No N/A values found in columns.\")\n",
    "na_validation(ct_nyc, COLS_ALLOWED_NA_VALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 13:42:53 - analysis-df-assembly - INFO - Dropped columns: {'n_images_by_area_p_alop', 'tract_id_ct', 'n_images_by_area_ct', 'tract_id_p_alop', 'tract_id'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# final cleaning \n",
    "TO_DROP = ['tract_id', 'n_images_by_area_']\n",
    "# drop all columns that match regex of entry in list \n",
    "current_cols = ct_nyc.columns\n",
    "for c in TO_DROP:\n",
    "    ct_nyc = ct_nyc.loc[:, ~ct_nyc.columns.str.contains(c)]\n",
    "\n",
    "logger.info(f\"Dropped columns: {set(current_cols) - set(ct_nyc.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2325.000000\n",
       "mean      3708.587957\n",
       "std       2059.971075\n",
       "min          0.000000\n",
       "25%       2305.000000\n",
       "50%       3447.000000\n",
       "75%       4876.000000\n",
       "max      15945.000000\n",
       "Name: total_population, dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc['total_population'].astype(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = pd.to_datetime('today').strftime('%m%d%Y')\n",
    "ct_nyc.to_csv(f'analysis_df_{todays_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
