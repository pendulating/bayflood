{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:43 - analysis-df-assembly - INFO - Modules loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import numpy as np \n",
    "import json \n",
    "from glob import glob \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(\"analysis-df-assembly\")\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "WGS='EPSG:4326'\n",
    "PROJ='EPSG:2263'\n",
    "\n",
    "import os \n",
    "\n",
    "logger.info(\"Modules loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERNAL_DATA = True \n",
    "\n",
    "USE_SMOOTHING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAR_NONE_RUN='../runs/icar_none/simulated_False/ahl_True/20241021-1038'\n",
    "ICAR_CHEATING_RUN='../runs/icar_cheating/simulated_False/ahl_True/20241022-1130'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:43 - analysis-df-assembly - INFO - Found 2 ICAR_NONE estimates and 3 ICAR_CHEATING estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ICAR_NONE_ESTIMATES = glob(f\"{ICAR_NONE_RUN}/estimate*.csv\")\n",
    "ICAR_CHEATING_ESTIMATES = glob(f\"{ICAR_CHEATING_RUN}/estimate*.csv\")\n",
    "logger.info(f\"Found {len(ICAR_NONE_ESTIMATES)} ICAR_NONE estimates and {len(ICAR_CHEATING_ESTIMATES)} ICAR_CHEATING estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_cheating_estimates = {} \n",
    "for f in ICAR_CHEATING_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_cheating_estimates[os.path.splitext(os.path.basename(f))[0]] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_none_estimates = {} \n",
    "for f in ICAR_NONE_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_none_estimates[os.path.splitext(os.path.basename(f)[0])] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:43 - analysis-df-assembly - INFO - Using smoothed estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_SMOOTHING: \n",
    "    icar_model_estimates = icar_cheating_estimates\n",
    "    logger.info(\"Using smoothed estimates.\")\n",
    "else:\n",
    "    icar_model_estimates = icar_none_estimates\n",
    "    logger.info(\"Using unsmoothed estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:43 - analysis-df-assembly - INFO - Loaded NYC CT shapefile with 2327 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson')\n",
    "\n",
    "\n",
    "TO_DROP = ['OBJECTID','BoroCode','CT2020','CDEligibil','NTA2020','CDTA2020','Shape__Area','Shape__Length','geometry']\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "ct_nyc = ct_nyc.set_index('GEOID').astype(str)\n",
    "\n",
    "logger.info(f\"Loaded NYC CT shapefile with {len(ct_nyc.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CTLabel</th>\n",
       "      <th>BoroName</th>\n",
       "      <th>BoroCT2020</th>\n",
       "      <th>NTAName</th>\n",
       "      <th>CDTANAME</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36061000100</th>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000100</td>\n",
       "      <td>The Battery-Governors Island-Ellis Island-Libe...</td>\n",
       "      <td>MN01 Financial District-Tribeca (CD 1 Equivalent)</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061000201</th>\n",
       "      <td>2.01</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000201</td>\n",
       "      <td>Chinatown-Two Bridges</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061000600</th>\n",
       "      <td>6</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000600</td>\n",
       "      <td>Chinatown-Two Bridges</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061001401</th>\n",
       "      <td>14.01</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1001401</td>\n",
       "      <td>Lower East Side</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061001402</th>\n",
       "      <td>14.02</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1001402</td>\n",
       "      <td>Lower East Side</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085017600</th>\n",
       "      <td>176</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5017600</td>\n",
       "      <td>Annadale-Huguenot-Prince's Bay-Woodrow</td>\n",
       "      <td>SI03 South Shore (CD 3 Approximation)</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085022802</th>\n",
       "      <td>228.02</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5022802</td>\n",
       "      <td>Freshkills Park (North)</td>\n",
       "      <td>SI02 Mid-Island (CD 2 Approximation)</td>\n",
       "      <td>4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085029102</th>\n",
       "      <td>291.02</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5029102</td>\n",
       "      <td>New Springville-Willowbrook-Bulls Head-Travis</td>\n",
       "      <td>SI02 Mid-Island (CD 2 Approximation)</td>\n",
       "      <td>4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005016100</th>\n",
       "      <td>161</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2016100</td>\n",
       "      <td>Crotona Park East</td>\n",
       "      <td>BX03 Morrisania-Crotona Park East (CD 3 Approx...</td>\n",
       "      <td>4263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005016300</th>\n",
       "      <td>163</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2016300</td>\n",
       "      <td>Crotona Park</td>\n",
       "      <td>BX03 Morrisania-Crotona Park East (CD 3 Approx...</td>\n",
       "      <td>4263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CTLabel       BoroName BoroCT2020  \\\n",
       "GEOID                                           \n",
       "36061000100       1      Manhattan    1000100   \n",
       "36061000201    2.01      Manhattan    1000201   \n",
       "36061000600       6      Manhattan    1000600   \n",
       "36061001401   14.01      Manhattan    1001401   \n",
       "36061001402   14.02      Manhattan    1001402   \n",
       "...             ...            ...        ...   \n",
       "36085017600     176  Staten Island    5017600   \n",
       "36085022802  228.02  Staten Island    5022802   \n",
       "36085029102  291.02  Staten Island    5029102   \n",
       "36005016100     161          Bronx    2016100   \n",
       "36005016300     163          Bronx    2016300   \n",
       "\n",
       "                                                       NTAName  \\\n",
       "GEOID                                                            \n",
       "36061000100  The Battery-Governors Island-Ellis Island-Libe...   \n",
       "36061000201                              Chinatown-Two Bridges   \n",
       "36061000600                              Chinatown-Two Bridges   \n",
       "36061001401                                    Lower East Side   \n",
       "36061001402                                    Lower East Side   \n",
       "...                                                        ...   \n",
       "36085017600             Annadale-Huguenot-Prince's Bay-Woodrow   \n",
       "36085022802                            Freshkills Park (North)   \n",
       "36085029102      New Springville-Willowbrook-Bulls Head-Travis   \n",
       "36005016100                                  Crotona Park East   \n",
       "36005016300                                       Crotona Park   \n",
       "\n",
       "                                                      CDTANAME  PUMA  \n",
       "GEOID                                                                 \n",
       "36061000100  MN01 Financial District-Tribeca (CD 1 Equivalent)  4121  \n",
       "36061000201   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061000600   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061001401   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061001402   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "...                                                        ...   ...  \n",
       "36085017600              SI03 South Shore (CD 3 Approximation)  4503  \n",
       "36085022802               SI02 Mid-Island (CD 2 Approximation)  4502  \n",
       "36085029102               SI02 Mid-Island (CD 2 Approximation)  4502  \n",
       "36005016100  BX03 Morrisania-Crotona Park East (CD 3 Approx...  4263  \n",
       "36005016300  BX03 Morrisania-Crotona Park East (CD 3 Approx...  4263  \n",
       "\n",
       "[2327 rows x 6 columns]"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:44 - analysis-df-assembly - INFO - Loaded NYC CT (water clipped) shapefile with 2325 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc_clip = gpd.read_file('geo/data/ct-nyc-2020.geojson')\n",
    "logger.info(f\"Loaded NYC CT (water clipped) shapefile with {len(ct_nyc_clip.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:44 - analysis-df-assembly - INFO - Merged NYC CT shapefile with icar model estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_p_y'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_y')).set_index('tract_id')\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_alop')).set_index('tract_id')\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area_if_you_have_100_images'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_alop_100')).set_index('tract_id')\n",
    "\n",
    "# drop empirical_estimate_* cols \n",
    "TO_DROP = [c for c in ct_nyc.columns if 'empirical_estimate_' in c]\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Merged NYC CT shapefile with icar model estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dp05_nyc_md = pd.read_json('demo/data/acs22_dp05_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "dp05_nyc_md = pd.json_normalize(dp05_nyc_md['variables']).set_index(dp05_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    dp05_nyc_md[f'desc_{i}'] = dp05_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP \n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "dp05_nyc_md = dp05_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "dp05_nyc_md = dp05_nyc_md[dp05_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows \n",
    "dp05_nyc_md = dp05_nyc_md.sort_index()\n",
    "\n",
    "# to csv \n",
    "dp05_nyc_md.to_csv('demo/data/acs22_dp05_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_population</th>\n",
       "      <th>nhl_white_alone</th>\n",
       "      <th>nhl_black_alone</th>\n",
       "      <th>hispanic_alone</th>\n",
       "      <th>nhl_asian_alone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>4446</td>\n",
       "      <td>1098</td>\n",
       "      <td>2000</td>\n",
       "      <td>1172</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>4870</td>\n",
       "      <td>83</td>\n",
       "      <td>1281</td>\n",
       "      <td>3109</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>6257</td>\n",
       "      <td>283</td>\n",
       "      <td>1559</td>\n",
       "      <td>4212</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>6177</td>\n",
       "      <td>106</td>\n",
       "      <td>2132</td>\n",
       "      <td>3507</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>2181</td>\n",
       "      <td>306</td>\n",
       "      <td>942</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>6374</td>\n",
       "      <td>2209</td>\n",
       "      <td>1568</td>\n",
       "      <td>1625</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>3674</td>\n",
       "      <td>289</td>\n",
       "      <td>1626</td>\n",
       "      <td>1469</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>5053</td>\n",
       "      <td>473</td>\n",
       "      <td>2388</td>\n",
       "      <td>1913</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>1133</td>\n",
       "      <td>109</td>\n",
       "      <td>421</td>\n",
       "      <td>394</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            total_population  nhl_white_alone  nhl_black_alone  \\\n",
       "tract_id                                                          \n",
       "36005000100              4446             1098             2000   \n",
       "36005000200              4870               83             1281   \n",
       "36005000400              6257              283             1559   \n",
       "36005001600              6177              106             2132   \n",
       "36005001901              2181              306              942   \n",
       "...                       ...              ...              ...   \n",
       "36085030302              6374             2209             1568   \n",
       "36085031901              3674              289             1626   \n",
       "36085031902              5053              473             2388   \n",
       "36085032300              1133              109              421   \n",
       "36085990100                 0                0                0   \n",
       "\n",
       "0            hispanic_alone  nhl_asian_alone  \n",
       "tract_id                                      \n",
       "36005000100            1172              123  \n",
       "36005000200            3109              299  \n",
       "36005000400            4212              103  \n",
       "36005001600            3507              148  \n",
       "36005001901             842                0  \n",
       "...                     ...              ...  \n",
       "36085030302            1625              918  \n",
       "36085031901            1469              224  \n",
       "36085031902            1913              217  \n",
       "36085032300             394               21  \n",
       "36085990100               0                0  \n",
       "\n",
       "[2327 rows x 5 columns]"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp05_nyc = pd.read_json('demo/data/acs22_dp05.json', orient='records')\n",
    "\n",
    "dp05_nyc.columns = dp05_nyc.iloc[0]\n",
    "dp05_nyc = dp05_nyc[1:]\n",
    "\n",
    "dp05_nyc['tract_id'] = dp05_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "RACE_COLS = {\n",
    "    'DP05_0001E': 'total_population',\n",
    "    'DP05_0079E': 'nhl_white_alone', \n",
    "    'DP05_0080E': 'nhl_black_alone', \n",
    "    'DP05_0073E': 'hispanic_alone', \n",
    "    'DP05_0082E': 'nhl_asian_alone'\n",
    "}\n",
    "\n",
    "race_nyc = dp05_nyc[list(RACE_COLS.keys())]\n",
    "race_nyc.columns = race_nyc.columns.map(lambda x: RACE_COLS[x])\n",
    "race_nyc.index = dp05_nyc['tract_id']\n",
    "# cast all columns to ints \n",
    "race_nyc = race_nyc.astype(int)\n",
    "race_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.merge(race_nyc, left_index=True, right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "s2801_nyc_md = pd.read_json('demo/data/acs22_s2801_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "s2801_nyc_md = pd.json_normalize(s2801_nyc_md['variables']).set_index(s2801_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(s2801_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(s2801_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    s2801_nyc_md[f'desc_{i}'] = s2801_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP \n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "s2801_nyc_md = s2801_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "s2801_nyc_md = s2801_nyc_md[s2801_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows \n",
    "s2801_nyc_md = s2801_nyc_md.sort_index()\n",
    "\n",
    "# to csv \n",
    "s2801_nyc_md.to_csv('demo/data/acs22_s2801_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_households_with_internet</th>\n",
       "      <th>num_households_with_smartphone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>1251</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>2264</td>\n",
       "      <td>2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>1843</td>\n",
       "      <td>2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>825</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>1994</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>885</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>1294</td>\n",
       "      <td>1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>334</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            num_households_with_internet  num_households_with_smartphone\n",
       "tract_id                                                                 \n",
       "36005000100                             0                               0\n",
       "36005000200                          1251                            1384\n",
       "36005000400                          2264                            2148\n",
       "36005001600                          1843                            2035\n",
       "36005001901                           825                             924\n",
       "...                                   ...                             ...\n",
       "36085030302                          1994                            2029\n",
       "36085031901                           885                            1078\n",
       "36085031902                          1294                            1445\n",
       "36085032300                           334                             365\n",
       "36085990100                             0                               0\n",
       "\n",
       "[2327 rows x 2 columns]"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2801_nyc = pd.read_json('demo/data/acs22_s2801.json', orient='records')\n",
    "\n",
    "s2801_nyc.columns = s2801_nyc.iloc[0]\n",
    "s2801_nyc = s2801_nyc[1:]\n",
    "\n",
    "s2801_nyc['tract_id'] = s2801_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "INTERNET_ACCESS_COLS = { \n",
    "    'S2801_C01_012E': 'num_households_with_internet',\n",
    "    'S2801_C01_005E': 'num_households_with_smartphone'\n",
    "}\n",
    "\n",
    "internet_access_nyc = s2801_nyc[list(INTERNET_ACCESS_COLS.keys())]\n",
    "internet_access_nyc.columns = internet_access_nyc.columns.map(lambda x: INTERNET_ACCESS_COLS[x])\n",
    "internet_access_nyc.index = s2801_nyc['tract_id']\n",
    "# cast all columns to ints\n",
    "internet_access_nyc = internet_access_nyc.astype(int)\n",
    "internet_access_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.join(internet_access_nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1901_nyc_md = pd.read_json('demo/data/acs22_s1901_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "s1901_nyc_md = pd.json_normalize(s1901_nyc_md['variables']).set_index(s1901_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(s1901_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(s1901_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    s1901_nyc_md[f'desc_{i}'] = s1901_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP\n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "s1901_nyc_md = s1901_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "s1901_nyc_md = s1901_nyc_md[s1901_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows\n",
    "s1901_nyc_md = s1901_nyc_md.sort_index()\n",
    "s1901_nyc_md.to_csv('demo/data/acs22_s1901_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_household_income</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>-666666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>115064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>100553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>41362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>49500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>98535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>45942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>75057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>88558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>-666666666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            median_household_income\n",
       "tract_id                            \n",
       "36005000100               -666666666\n",
       "36005000200                   115064\n",
       "36005000400                   100553\n",
       "36005001600                    41362\n",
       "36005001901                    49500\n",
       "...                              ...\n",
       "36085030302                    98535\n",
       "36085031901                    45942\n",
       "36085031902                    75057\n",
       "36085032300                    88558\n",
       "36085990100               -666666666\n",
       "\n",
       "[2327 rows x 1 columns]"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1901_nyc = pd.read_json('demo/data/acs22_s1901.json', orient='records')\n",
    "\n",
    "s1901_nyc.columns = s1901_nyc.iloc[0]\n",
    "s1901_nyc = s1901_nyc[1:]\n",
    "\n",
    "s1901_nyc['tract_id'] = s1901_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "INCOME_COLS = {\n",
    "    'S1901_C01_012E': 'median_household_income'\n",
    "}\n",
    "\n",
    "income_nyc = s1901_nyc[list(INCOME_COLS.keys())]\n",
    "income_nyc.columns = income_nyc.columns.map(lambda x: INCOME_COLS[x])\n",
    "income_nyc.index = s1901_nyc['tract_id']\n",
    "\n",
    "# cast all columns to ints\n",
    "income_nyc = income_nyc.astype(int)\n",
    "income_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.join(income_nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Educational Attainment \n",
    "s1501_nyc_md = pd.read_json('demo/data/acs22_s1501_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "s1501_nyc_md = pd.json_normalize(s1501_nyc_md['variables']).set_index(s1501_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(s1501_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(s1501_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    s1501_nyc_md[f'desc_{i}'] = s1501_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP\n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "s1501_nyc_md = s1501_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "s1501_nyc_md = s1501_nyc_md[s1501_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows\n",
    "s1501_nyc_md = s1501_nyc_md.sort_index()\n",
    "s1501_nyc_md.to_csv('demo/data/acs22_s1501_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_high_school_graduates</th>\n",
       "      <th>num_bachelors_degree</th>\n",
       "      <th>num_graduate_degree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>1835</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>1089</td>\n",
       "      <td>489</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>878</td>\n",
       "      <td>995</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>1101</td>\n",
       "      <td>682</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>408</td>\n",
       "      <td>283</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>1357</td>\n",
       "      <td>1069</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>549</td>\n",
       "      <td>185</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>1020</td>\n",
       "      <td>677</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>287</td>\n",
       "      <td>106</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            num_high_school_graduates  num_bachelors_degree  \\\n",
       "tract_id                                                       \n",
       "36005000100                       1835                    34   \n",
       "36005000200                       1089                   489   \n",
       "36005000400                        878                   995   \n",
       "36005001600                       1101                   682   \n",
       "36005001901                        408                   283   \n",
       "...                                ...                   ...   \n",
       "36085030302                       1357                  1069   \n",
       "36085031901                        549                   185   \n",
       "36085031902                       1020                   677   \n",
       "36085032300                        287                   106   \n",
       "36085990100                          0                     0   \n",
       "\n",
       "0            num_graduate_degree  \n",
       "tract_id                          \n",
       "36005000100                   41  \n",
       "36005000200                  671  \n",
       "36005000400                  544  \n",
       "36005001600                  123  \n",
       "36005001901                  164  \n",
       "...                          ...  \n",
       "36085030302                  430  \n",
       "36085031901                  175  \n",
       "36085031902                  241  \n",
       "36085032300                  153  \n",
       "36085990100                    0  \n",
       "\n",
       "[2327 rows x 3 columns]"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1501_nyc = pd.read_json('demo/data/acs22_s1501.json', orient='records')\n",
    "\n",
    "s1501_nyc.columns = s1501_nyc.iloc[0]\n",
    "s1501_nyc = s1501_nyc[1:]\n",
    "\n",
    "s1501_nyc['tract_id'] = s1501_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "EDUCATION_COLS = {\n",
    "    'S1501_C01_009E': 'num_high_school_graduates',\n",
    "    'S1501_C01_012E': 'num_bachelors_degree',\n",
    "    'S1501_C01_013E': 'num_graduate_degree'\n",
    "}\n",
    "\n",
    "education_nyc = s1501_nyc[list(EDUCATION_COLS.keys())]\n",
    "education_nyc.columns = education_nyc.columns.map(lambda x: EDUCATION_COLS[x])\n",
    "education_nyc.index = s1501_nyc['tract_id']\n",
    "\n",
    "# cast all columns to ints\n",
    "education_nyc = education_nyc.astype(int)\n",
    "education_nyc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limited english speaking households \n",
    "s1602_nyc_md = pd.read_json('demo/data/acs22_s1602_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "s1602_nyc_md = pd.json_normalize(s1602_nyc_md['variables']).set_index(s1602_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(s1602_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(s1602_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    s1602_nyc_md[f'desc_{i}'] = s1602_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP\n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "s1602_nyc_md = s1602_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "\n",
    "s1602_nyc_md = s1602_nyc_md[s1602_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows\n",
    "s1602_nyc_md = s1602_nyc_md.sort_index()\n",
    "s1602_nyc_md.to_csv('demo/data/acs22_s1602_md.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.join(education_nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_limited_english_speaking_households</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            num_limited_english_speaking_households\n",
       "tract_id                                            \n",
       "36005000100                                        0\n",
       "36005000200                                      257\n",
       "36005000400                                      445\n",
       "36005001600                                      336\n",
       "36005001901                                       55\n",
       "...                                              ...\n",
       "36085030302                                       86\n",
       "36085031901                                       46\n",
       "36085031902                                       87\n",
       "36085032300                                        9\n",
       "36085990100                                        0\n",
       "\n",
       "[2327 rows x 1 columns]"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1602_nyc_md = pd.read_csv('demo/data/acs22_s1602_md.csv')\n",
    "\n",
    "s1602_nyc = pd.read_json('demo/data/acs22_s1602.json', orient='records')\n",
    "\n",
    "s1602_nyc.columns = s1602_nyc.iloc[0]\n",
    "s1602_nyc = s1602_nyc[1:]\n",
    "\n",
    "s1602_nyc['tract_id'] = s1602_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "LEH_COLS = {\n",
    "    'S1602_C03_001E': 'num_limited_english_speaking_households'\n",
    "}\n",
    "\n",
    "leh_nyc = s1602_nyc[list(LEH_COLS.keys())]\n",
    "leh_nyc.columns = leh_nyc.columns.map(lambda x: LEH_COLS[x])\n",
    "leh_nyc.index = s1602_nyc['tract_id']\n",
    "\n",
    "# cast all columns to ints\n",
    "leh_nyc = leh_nyc.astype(int)\n",
    "leh_nyc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.join(leh_nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPOLOGY \n",
    "topology_ct_nyc = pd.read_csv('geo/data/processed/ct_nyc_topology.csv', index_col=0)\n",
    "topology_ct_nyc['GEOID'] = topology_ct_nyc['GEOID'].astype(str)\n",
    "topology_ct_nyc = topology_ct_nyc.set_index('GEOID')\n",
    "# prepend columns with 'ft_elevation'\n",
    "topology_ct_nyc.columns = ['ft_elevation_' + c for c in topology_ct_nyc.columns]\n",
    "ct_nyc = ct_nyc.merge(topology_ct_nyc, left_index=True, right_on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson').to_crs(PROJ)[['GEOID','geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic geographic features \n",
    "gdf_ct_nyc['area'] = gdf_ct_nyc.area \n",
    "# add this col to ct_nyc\n",
    "ct_nyc = ct_nyc.merge(gdf_ct_nyc[['GEOID','area']], left_index=True, right_on='GEOID').set_index('GEOID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:46 - analysis-df-assembly - INFO - Loaded and processed Floodnet sensor data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FLOODNET \n",
    "floodnet_sensor = pd.read_csv('flooding/static/floodnet-flood-sensor-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_tide = pd.read_csv('flooding/static/floodnet-tide-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_weather = pd.read_csv('flooding/static/floodnet-weather-sep-2023.csv', engine='pyarrow')\n",
    "\n",
    "\n",
    "all_floodnet_sensors_geo = pd.concat([floodnet_sensor.groupby('deployment_id').first()[['lat','lon']].reset_index(), floodnet_tide.groupby('sensor_id').first()[['lat','lon']].reset_index(), floodnet_weather.groupby('sensor_id').first()[['lat','lon']].reset_index()], axis=0)\n",
    "\n",
    "all_floodnet_sensor_geo = gpd.GeoDataFrame(all_floodnet_sensors_geo, geometry=gpd.points_from_xy(all_floodnet_sensors_geo.lon, all_floodnet_sensors_geo.lat), crs='EPSG:4326').to_crs(2263)\n",
    "\n",
    "del floodnet_sensor, floodnet_tide, floodnet_weather\n",
    "\n",
    "logger.info(\"Loaded and processed Floodnet sensor data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:46 - analysis-df-assembly - INFO - Merged Floodnet sensor data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 16:51:46 - analysis-df-assembly - INFO - count    2325.000000\n",
      "mean        0.036989\n",
      "std         0.274269\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         6.000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the number of floodnet sensors in each census tract in gdf_ct_nyc, including tracts with 0 sensors\n",
    "ct_nyc['n_sensors'] = gpd.sjoin(gdf_ct_nyc, all_floodnet_sensor_geo).groupby('GEOID').size().reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged Floodnet sensor data with NYC CT shapefile.\")\n",
    "logger.info(ct_nyc['n_sensors'].describe().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEP STORMWATER \n",
    "dep_moderate = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Moderate Flood (2.13 inches per hr) with Current Sea Levels/NYC_Stormwater_Flood_Map_Moderate_Flood_2_13_inches_per_hr_with_Current_Sea_Levels.gdb').to_crs(PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {}\n",
    "for i, row in dep_moderate.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "    \n",
    "# dataframe from dict\n",
    "dep_moderate_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "\n",
    "dep_moderate_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_moderate_flattened.crs = dep_moderate.crs\n",
    "\n",
    "dep_moderate_flattened['Flooding_Category'] = dep_moderate_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:49 - analysis-df-assembly - INFO - Merged DEP stormwater data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 16:51:49 - analysis-df-assembly - INFO - \n",
      "       dep_moderate_1_area  dep_moderate_2_area  dep_moderate_1_frac  dep_moderate_2_frac\n",
      "count          2325.000000          2325.000000          2325.000000          2325.000000\n",
      "mean          33568.861141         17255.143767             0.009435             0.004953\n",
      "std           77948.072724         49174.975178             0.016486             0.014127\n",
      "min               0.000000             0.000000             0.000000             0.000000\n",
      "25%               0.000000             0.000000             0.000000             0.000000\n",
      "50%            5211.088807             0.000000             0.001501             0.000000\n",
      "75%           33654.810667          9705.671683             0.012128             0.003431\n",
      "max          886875.925303        660828.326813             0.150287             0.254063\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the total area of light and moderate flooding in each ct in gdf_ct_nyc \n",
    "dep_moderate_flattened_1 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 1]\n",
    "dep_moderate_flattened_2 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 2]\n",
    "ct_nyc['dep_moderate_1_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_moderate_2_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "ct_nyc['dep_moderate_1_frac'] = ct_nyc['dep_moderate_1_area'] / ct_nyc['area']\n",
    "ct_nyc['dep_moderate_2_frac'] = ct_nyc['dep_moderate_2_area'] / ct_nyc['area']\n",
    "\n",
    "logger.info(\"Merged DEP stormwater data with NYC CT shapefile.\")\n",
    "\n",
    "logger.info(\"\\n\"+ct_nyc[['dep_moderate_1_area','dep_moderate_2_area','dep_moderate_1_frac','dep_moderate_2_frac']].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:49 - analysis-df-assembly - INFO - Loaded and processed 311 data from September 29, 2023.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 311 from September 29 Flood \n",
    "nyc311_sep29 = pd.read_csv('flooding/data/nyc311_flooding_sep29.csv').dropna(subset=['latitude','longitude'])\n",
    "\n",
    "nyc311_sep29 = gpd.GeoDataFrame(nyc311_sep29, geometry=gpd.points_from_xy(nyc311_sep29.longitude, nyc311_sep29.latitude), crs=WGS).to_crs(PROJ)\n",
    "\n",
    "logger.info(\"Loaded and processed 311 data from September 29, 2023.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:53 - analysis-df-assembly - INFO - Merged 311 data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 16:51:53 - analysis-df-assembly - INFO - \n",
      "sewer_backup_311c                    1081\n",
      "street_flooding_311c                  625\n",
      "catch_basin_clogged/flooding_311c     429\n",
      "manhole_overflow_311c                  35\n",
      "highway_flooding_311c                   1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for each unique val of descriptor, create a column in gdf_ct_nyc with the count of 311 calls of that descriptor type in each tract \n",
    "for descriptor in nyc311_sep29['descriptor'].unique():\n",
    "    # human-readable column name \n",
    "    # remove anything inside () \n",
    "    desc = descriptor.split('(')[0].strip()\n",
    "    desc = desc.lower().replace(' ', '_') + '_311c'\n",
    "\n",
    "    gdf_ct_nyc[desc] = gdf_ct_nyc['geometry'].apply(lambda x: nyc311_sep29[nyc311_sep29['descriptor'] == descriptor].within(x).sum())\n",
    "\n",
    "logger.info(\"Merged 311 data with NYC CT shapefile.\")\n",
    "logger.info(\"\\n\"+ gdf_ct_nyc[[c for c in gdf_ct_nyc.columns if '_311c' in c]].sum().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ct_nyc with gdf_ct_nyc\n",
    "ct_nyc = ct_nyc.merge(gdf_ct_nyc, left_index=True, right_on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-23 16:51:53 - analysis-df-assembly - SUCCESS - No N/A values found in columns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "COLS_ALLOWED_NA_VALS = ['empirical_estimate']\n",
    "def na_validation(df, cols_allowed_na_vals):\n",
    "    for c in df.columns:\n",
    "        if c in cols_allowed_na_vals:\n",
    "            continue\n",
    "        if df[c].isna().sum() > 0:\n",
    "            logger.error(f\"Column {c} has {df[c].isna().sum()} NA values.\")\n",
    "    else: \n",
    "        logger.success(\"No N/A values found in columns.\")\n",
    "na_validation(ct_nyc, COLS_ALLOWED_NA_VALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 16:51:53 - analysis-df-assembly - INFO - Dropped columns: {'n_images_by_area_p_alop', 'n_images_by_area_ct'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# final cleaning \n",
    "TO_DROP = ['tract_id', 'n_images_by_area_']\n",
    "# drop all columns that match regex of entry in list \n",
    "current_cols = ct_nyc.columns\n",
    "for c in TO_DROP:\n",
    "    ct_nyc = ct_nyc.loc[:, ~ct_nyc.columns.str.contains(c)]\n",
    "\n",
    "logger.info(f\"Dropped columns: {set(current_cols) - set(ct_nyc.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = pd.to_datetime('today').strftime('%m%d%Y')\n",
    "ct_nyc.to_csv(f'analysis_df_{todays_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
