{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:09 - analysis-df-assembly - INFO - Modules loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import numpy as np \n",
    "import json \n",
    "from glob import glob \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(\"analysis-df-assembly\")\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "WGS='EPSG:4326'\n",
    "PROJ='EPSG:2263'\n",
    "\n",
    "import os \n",
    "\n",
    "logger.info(\"Modules loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERNAL_DATA = True \n",
    "\n",
    "USE_SMOOTHING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAR_NONE_RUN='../runs/icar_none/simulated_False/ahl_True/20241021-1038'\n",
    "ICAR_CHEATING_RUN='../runs/icar_cheating/simulated_False/ahl_True/20241022-1130'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:09 - analysis-df-assembly - INFO - Found 2 ICAR_NONE estimates and 3 ICAR_CHEATING estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ICAR_NONE_ESTIMATES = glob(f\"{ICAR_NONE_RUN}/estimate*.csv\")\n",
    "ICAR_CHEATING_ESTIMATES = glob(f\"{ICAR_CHEATING_RUN}/estimate*.csv\")\n",
    "logger.info(f\"Found {len(ICAR_NONE_ESTIMATES)} ICAR_NONE estimates and {len(ICAR_CHEATING_ESTIMATES)} ICAR_CHEATING estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_cheating_estimates = {} \n",
    "for f in ICAR_CHEATING_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_cheating_estimates[os.path.splitext(os.path.basename(f))[0]] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_none_estimates = {} \n",
    "for f in ICAR_NONE_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_none_estimates[os.path.splitext(os.path.basename(f)[0])] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:09 - analysis-df-assembly - INFO - Using smoothed estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_SMOOTHING: \n",
    "    icar_model_estimates = icar_cheating_estimates\n",
    "    logger.info(\"Using smoothed estimates.\")\n",
    "else:\n",
    "    icar_model_estimates = icar_none_estimates\n",
    "    logger.info(\"Using unsmoothed estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:09 - analysis-df-assembly - INFO - Loaded NYC CT shapefile with 2327 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson')\n",
    "\n",
    "\n",
    "TO_DROP = ['OBJECTID','BoroCode','CT2020','CDEligibil','NTA2020','CDTA2020','Shape__Area','Shape__Length','geometry']\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "ct_nyc = ct_nyc.set_index('GEOID').astype(str)\n",
    "\n",
    "logger.info(f\"Loaded NYC CT shapefile with {len(ct_nyc.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CTLabel</th>\n",
       "      <th>BoroName</th>\n",
       "      <th>BoroCT2020</th>\n",
       "      <th>NTAName</th>\n",
       "      <th>CDTANAME</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36061000100</th>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000100</td>\n",
       "      <td>The Battery-Governors Island-Ellis Island-Libe...</td>\n",
       "      <td>MN01 Financial District-Tribeca (CD 1 Equivalent)</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061000201</th>\n",
       "      <td>2.01</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000201</td>\n",
       "      <td>Chinatown-Two Bridges</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061000600</th>\n",
       "      <td>6</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1000600</td>\n",
       "      <td>Chinatown-Two Bridges</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061001401</th>\n",
       "      <td>14.01</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1001401</td>\n",
       "      <td>Lower East Side</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061001402</th>\n",
       "      <td>14.02</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1001402</td>\n",
       "      <td>Lower East Side</td>\n",
       "      <td>MN03 Lower East Side-Chinatown (CD 3 Equivalent)</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085017600</th>\n",
       "      <td>176</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5017600</td>\n",
       "      <td>Annadale-Huguenot-Prince's Bay-Woodrow</td>\n",
       "      <td>SI03 South Shore (CD 3 Approximation)</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085022802</th>\n",
       "      <td>228.02</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5022802</td>\n",
       "      <td>Freshkills Park (North)</td>\n",
       "      <td>SI02 Mid-Island (CD 2 Approximation)</td>\n",
       "      <td>4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085029102</th>\n",
       "      <td>291.02</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>5029102</td>\n",
       "      <td>New Springville-Willowbrook-Bulls Head-Travis</td>\n",
       "      <td>SI02 Mid-Island (CD 2 Approximation)</td>\n",
       "      <td>4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005016100</th>\n",
       "      <td>161</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2016100</td>\n",
       "      <td>Crotona Park East</td>\n",
       "      <td>BX03 Morrisania-Crotona Park East (CD 3 Approx...</td>\n",
       "      <td>4263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005016300</th>\n",
       "      <td>163</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2016300</td>\n",
       "      <td>Crotona Park</td>\n",
       "      <td>BX03 Morrisania-Crotona Park East (CD 3 Approx...</td>\n",
       "      <td>4263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CTLabel       BoroName BoroCT2020  \\\n",
       "GEOID                                           \n",
       "36061000100       1      Manhattan    1000100   \n",
       "36061000201    2.01      Manhattan    1000201   \n",
       "36061000600       6      Manhattan    1000600   \n",
       "36061001401   14.01      Manhattan    1001401   \n",
       "36061001402   14.02      Manhattan    1001402   \n",
       "...             ...            ...        ...   \n",
       "36085017600     176  Staten Island    5017600   \n",
       "36085022802  228.02  Staten Island    5022802   \n",
       "36085029102  291.02  Staten Island    5029102   \n",
       "36005016100     161          Bronx    2016100   \n",
       "36005016300     163          Bronx    2016300   \n",
       "\n",
       "                                                       NTAName  \\\n",
       "GEOID                                                            \n",
       "36061000100  The Battery-Governors Island-Ellis Island-Libe...   \n",
       "36061000201                              Chinatown-Two Bridges   \n",
       "36061000600                              Chinatown-Two Bridges   \n",
       "36061001401                                    Lower East Side   \n",
       "36061001402                                    Lower East Side   \n",
       "...                                                        ...   \n",
       "36085017600             Annadale-Huguenot-Prince's Bay-Woodrow   \n",
       "36085022802                            Freshkills Park (North)   \n",
       "36085029102      New Springville-Willowbrook-Bulls Head-Travis   \n",
       "36005016100                                  Crotona Park East   \n",
       "36005016300                                       Crotona Park   \n",
       "\n",
       "                                                      CDTANAME  PUMA  \n",
       "GEOID                                                                 \n",
       "36061000100  MN01 Financial District-Tribeca (CD 1 Equivalent)  4121  \n",
       "36061000201   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061000600   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061001401   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "36061001402   MN03 Lower East Side-Chinatown (CD 3 Equivalent)  4103  \n",
       "...                                                        ...   ...  \n",
       "36085017600              SI03 South Shore (CD 3 Approximation)  4503  \n",
       "36085022802               SI02 Mid-Island (CD 2 Approximation)  4502  \n",
       "36085029102               SI02 Mid-Island (CD 2 Approximation)  4502  \n",
       "36005016100  BX03 Morrisania-Crotona Park East (CD 3 Approx...  4263  \n",
       "36005016300  BX03 Morrisania-Crotona Park East (CD 3 Approx...  4263  \n",
       "\n",
       "[2327 rows x 6 columns]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:10 - analysis-df-assembly - INFO - Loaded NYC CT (water clipped) shapefile with 2325 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc_clip = gpd.read_file('geo/data/ct-nyc-2020.geojson')\n",
    "logger.info(f\"Loaded NYC CT (water clipped) shapefile with {len(ct_nyc_clip.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:10 - analysis-df-assembly - INFO - Merged NYC CT shapefile with icar model estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_p_y'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_y')).set_index('tract_id')\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_alop')).set_index('tract_id')\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area_if_you_have_100_images'], left_index=True, right_on='tract_id', suffixes=('_ct', '_p_alop_100')).set_index('tract_id')\n",
    "\n",
    "# drop empirical_estimate_* cols \n",
    "TO_DROP = [c for c in ct_nyc.columns if 'empirical_estimate_' in c]\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Merged NYC CT shapefile with icar model estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dp05_nyc_md = pd.read_json('demo/data/acs22_dp05_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "dp05_nyc_md = pd.json_normalize(dp05_nyc_md['variables']).set_index(dp05_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    dp05_nyc_md[f'desc_{i}'] = dp05_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP \n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "dp05_nyc_md = dp05_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "dp05_nyc_md = dp05_nyc_md[dp05_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows \n",
    "dp05_nyc_md = dp05_nyc_md.sort_index()\n",
    "\n",
    "# to csv \n",
    "dp05_nyc_md.to_csv('demo/data/acs22_dp05_md.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_population</th>\n",
       "      <th>nhl_white_alone</th>\n",
       "      <th>nhl_black_alone</th>\n",
       "      <th>hispanic_alone</th>\n",
       "      <th>nhl_asian_alone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>4446</td>\n",
       "      <td>1098</td>\n",
       "      <td>2000</td>\n",
       "      <td>1172</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>4870</td>\n",
       "      <td>83</td>\n",
       "      <td>1281</td>\n",
       "      <td>3109</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>6257</td>\n",
       "      <td>283</td>\n",
       "      <td>1559</td>\n",
       "      <td>4212</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>6177</td>\n",
       "      <td>106</td>\n",
       "      <td>2132</td>\n",
       "      <td>3507</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>2181</td>\n",
       "      <td>306</td>\n",
       "      <td>942</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>6374</td>\n",
       "      <td>2209</td>\n",
       "      <td>1568</td>\n",
       "      <td>1625</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>3674</td>\n",
       "      <td>289</td>\n",
       "      <td>1626</td>\n",
       "      <td>1469</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>5053</td>\n",
       "      <td>473</td>\n",
       "      <td>2388</td>\n",
       "      <td>1913</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>1133</td>\n",
       "      <td>109</td>\n",
       "      <td>421</td>\n",
       "      <td>394</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0            total_population  nhl_white_alone  nhl_black_alone  \\\n",
       "tract_id                                                          \n",
       "36005000100              4446             1098             2000   \n",
       "36005000200              4870               83             1281   \n",
       "36005000400              6257              283             1559   \n",
       "36005001600              6177              106             2132   \n",
       "36005001901              2181              306              942   \n",
       "...                       ...              ...              ...   \n",
       "36085030302              6374             2209             1568   \n",
       "36085031901              3674              289             1626   \n",
       "36085031902              5053              473             2388   \n",
       "36085032300              1133              109              421   \n",
       "36085990100                 0                0                0   \n",
       "\n",
       "0            hispanic_alone  nhl_asian_alone  \n",
       "tract_id                                      \n",
       "36005000100            1172              123  \n",
       "36005000200            3109              299  \n",
       "36005000400            4212              103  \n",
       "36005001600            3507              148  \n",
       "36005001901             842                0  \n",
       "...                     ...              ...  \n",
       "36085030302            1625              918  \n",
       "36085031901            1469              224  \n",
       "36085031902            1913              217  \n",
       "36085032300             394               21  \n",
       "36085990100               0                0  \n",
       "\n",
       "[2327 rows x 5 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp05_nyc = pd.read_json('demo/data/acs22_dp05.json', orient='records')\n",
    "\n",
    "dp05_nyc.columns = dp05_nyc.iloc[0]\n",
    "dp05_nyc = dp05_nyc[1:]\n",
    "\n",
    "dp05_nyc['tract_id'] = dp05_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "RACE_COLS = {\n",
    "    'DP05_0001E': 'total_population',\n",
    "    'DP05_0079E': 'nhl_white_alone', \n",
    "    'DP05_0080E': 'nhl_black_alone', \n",
    "    'DP05_0073E': 'hispanic_alone', \n",
    "    'DP05_0082E': 'nhl_asian_alone'\n",
    "}\n",
    "\n",
    "race_nyc = dp05_nyc[list(RACE_COLS.keys())]\n",
    "race_nyc.columns = race_nyc.columns.map(lambda x: RACE_COLS[x])\n",
    "race_nyc.index = dp05_nyc['tract_id']\n",
    "# cast all columns to ints \n",
    "race_nyc = race_nyc.astype(int)\n",
    "race_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.merge(race_nyc, left_index=True, right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson').to_crs(PROJ)[['GEOID','geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:12 - analysis-df-assembly - INFO - Loaded and processed Floodnet sensor data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FLOODNET \n",
    "floodnet_sensor = pd.read_csv('flooding/static/floodnet-flood-sensor-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_tide = pd.read_csv('flooding/static/floodnet-tide-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_weather = pd.read_csv('flooding/static/floodnet-weather-sep-2023.csv', engine='pyarrow')\n",
    "\n",
    "\n",
    "all_floodnet_sensors_geo = pd.concat([floodnet_sensor.groupby('deployment_id').first()[['lat','lon']].reset_index(), floodnet_tide.groupby('sensor_id').first()[['lat','lon']].reset_index(), floodnet_weather.groupby('sensor_id').first()[['lat','lon']].reset_index()], axis=0)\n",
    "\n",
    "all_floodnet_sensor_geo = gpd.GeoDataFrame(all_floodnet_sensors_geo, geometry=gpd.points_from_xy(all_floodnet_sensors_geo.lon, all_floodnet_sensors_geo.lat), crs='EPSG:4326').to_crs(2263)\n",
    "\n",
    "del floodnet_sensor, floodnet_tide, floodnet_weather\n",
    "\n",
    "logger.info(\"Loaded and processed Floodnet sensor data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:16:12 - analysis-df-assembly - INFO - Merged Floodnet sensor data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 14:16:12 - analysis-df-assembly - INFO - count    2327.000000\n",
      "mean        0.036957\n",
      "std         0.274153\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         6.000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the number of floodnet sensors in each census tract in gdf_ct_nyc, including tracts with 0 sensors\n",
    "ct_nyc['n_sensors'] = gpd.sjoin(gdf_ct_nyc, all_floodnet_sensor_geo).groupby('GEOID').size().reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged Floodnet sensor data with NYC CT shapefile.\")\n",
    "logger.info(ct_nyc['n_sensors'].describe().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEP STORMWATER \n",
    "dep_light = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Extreme Flood (3.66 inches per hr) with 2080 Sea Level Rise/NYC_Stormwater_Flood_Map_Extreme_Flood_3_66_inches_per_hr_with_2080_Sea_Level_Rise.gdb').to_crs(PROJ)\n",
    "dep_moderate = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Moderate Flood (2.13 inches per hr) with Current Sea Levels/NYC_Stormwater_Flood_Map_Moderate_Flood_2_13_inches_per_hr_with_Current_Sea_Levels.gdb').to_crs(PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {} \n",
    "for i, row in dep_light.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "\n",
    "# dataframe from dict \n",
    "dep_light_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "# flooding category is the first part of the index\n",
    "\n",
    "\n",
    "dep_light_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_light_flattened.crs = dep_light.crs\n",
    "\n",
    "dep_light_flattened['Flooding_Category'] = dep_light_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {}\n",
    "for i, row in dep_moderate.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "    \n",
    "# dataframe from dict\n",
    "dep_moderate_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "\n",
    "dep_moderate_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_moderate_flattened.crs = dep_moderate.crs\n",
    "\n",
    "dep_moderate_flattened['Flooding_Category'] = dep_moderate_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:17:09 - analysis-df-assembly - INFO - Merged DEP stormwater data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 14:17:09 - analysis-df-assembly - INFO - \n",
      "       dep_light_1_area  dep_light_2_area  dep_light_3_area  dep_moderate_1_area  dep_moderate_2_area\n",
      "count      2.327000e+03      2.327000e+03      2.327000e+03          2327.000000          2327.000000\n",
      "mean       1.702803e+05      1.304646e+05      1.479647e+05         33540.009520         17240.313390\n",
      "std        1.926455e+05      2.014861e+05      8.544983e+05         77920.766203         49156.431065\n",
      "min        0.000000e+00      0.000000e+00      0.000000e+00             0.000000             0.000000\n",
      "25%        4.729919e+04      1.261551e+04      0.000000e+00             0.000000             0.000000\n",
      "50%        1.175676e+05      5.505134e+04      0.000000e+00          5169.673159             0.000000\n",
      "75%        2.224860e+05      1.642148e+05      0.000000e+00         33635.337057          9690.586180\n",
      "max        1.653186e+06      2.586616e+06      1.772139e+07        886875.925303        660828.326813\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the total area of light and moderate flooding in each ct in gdf_ct_nyc \n",
    "dep_light_flattened_1 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 1]\n",
    "dep_light_flattened_2 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 2]\n",
    "dep_light_flattened_3 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 3]\n",
    "ct_nyc['dep_light_1_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_2_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_3_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_3, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "dep_moderate_flattened_1 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 1]\n",
    "dep_moderate_flattened_2 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 2]\n",
    "ct_nyc['dep_moderate_1_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_moderate_2_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged DEP stormwater data with NYC CT shapefile.\")\n",
    "\n",
    "logger.info(\"\\n\"+ct_nyc[['dep_light_1_area','dep_light_2_area','dep_light_3_area','dep_moderate_1_area','dep_moderate_2_area']].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:17:09 - analysis-df-assembly - INFO - Loaded and processed 311 data from September 29, 2023.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 311 from September 29 Flood \n",
    "nyc311_sep29 = pd.read_csv('flooding/data/nyc311_flooding_sep29.csv').dropna(subset=['latitude','longitude'])\n",
    "\n",
    "nyc311_sep29 = gpd.GeoDataFrame(nyc311_sep29, geometry=gpd.points_from_xy(nyc311_sep29.longitude, nyc311_sep29.latitude), crs=WGS).to_crs(PROJ)\n",
    "\n",
    "logger.info(\"Loaded and processed 311 data from September 29, 2023.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:17:13 - analysis-df-assembly - INFO - Merged 311 data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 14:17:13 - analysis-df-assembly - INFO - \n",
      "sewer_backup_311c                    1081\n",
      "street_flooding_311c                  625\n",
      "catch_basin_clogged/flooding_311c     429\n",
      "manhole_overflow_311c                  35\n",
      "highway_flooding_311c                   1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for each unique val of descriptor, create a column in gdf_ct_nyc with the count of 311 calls of that descriptor type in each tract \n",
    "for descriptor in nyc311_sep29['descriptor'].unique():\n",
    "    # human-readable column name \n",
    "    # remove anything inside () \n",
    "    desc = descriptor.split('(')[0].strip()\n",
    "    desc = desc.lower().replace(' ', '_') + '_311c'\n",
    "\n",
    "    gdf_ct_nyc[desc] = gdf_ct_nyc['geometry'].apply(lambda x: nyc311_sep29[nyc311_sep29['descriptor'] == descriptor].within(x).sum())\n",
    "\n",
    "logger.info(\"Merged 311 data with NYC CT shapefile.\")\n",
    "logger.info(\"\\n\"+ gdf_ct_nyc[[c for c in gdf_ct_nyc.columns if '_311c' in c]].sum().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ct_nyc with gdf_ct_nyc\n",
    "ct_nyc = ct_nyc.merge(gdf_ct_nyc, left_index=True, right_on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-23 14:17:29 - analysis-df-assembly - SUCCESS - No N/A values found in columns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "COLS_ALLOWED_NA_VALS = ['empirical_estimate']\n",
    "def na_validation(df, cols_allowed_na_vals):\n",
    "    for c in df.columns:\n",
    "        if c in cols_allowed_na_vals:\n",
    "            continue\n",
    "        if df[c].isna().sum() > 0:\n",
    "            logger.error(f\"Column {c} has {df[c].isna().sum()} NA values.\")\n",
    "    else: \n",
    "        logger.success(\"No N/A values found in columns.\")\n",
    "na_validation(ct_nyc, COLS_ALLOWED_NA_VALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 14:17:30 - analysis-df-assembly - INFO - Dropped columns: {'n_images_by_area_p_alop', 'n_images_by_area_ct'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# final cleaning \n",
    "TO_DROP = ['tract_id', 'n_images_by_area_']\n",
    "# drop all columns that match regex of entry in list \n",
    "current_cols = ct_nyc.columns\n",
    "for c in TO_DROP:\n",
    "    ct_nyc = ct_nyc.loc[:, ~ct_nyc.columns.str.contains(c)]\n",
    "\n",
    "logger.info(f\"Dropped columns: {set(current_cols) - set(ct_nyc.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = pd.to_datetime('today').strftime('%m%d%Y')\n",
    "ct_nyc.to_csv(f'analysis_df_{todays_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
